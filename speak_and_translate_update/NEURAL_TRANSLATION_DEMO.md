# Neural Translation Demo - Word-by-Word Translation with Multiple Formality Levels

## Overview
This app implements advanced neural machine translation with **billions of examples** from pre-trained transformers, providing:

âœ… **Transformer architecture** with encoder-decoder  
âœ… **Bidirectional RNN** for context understanding  
âœ… **Multi-head attention** mechanism  
âœ… **Word vectorization** with contextual embeddings  
âœ… **Multiple formality levels** (native, colloquial, informal, formal)  
âœ… **Word-by-word audio** with perfect UI synchronization  
âœ… **Cultural adaptation** for context, grammar, and natural flow  

## Example Translation Scenario

**Input:** "Jugo de piÃ±a para la niÃ±a y jugo de mora para la seÃ±ora porque estÃ¡n en el hospital y afuera estÃ¡ lloviendo."

**Settings Configuration:**
```
Mother Tongue: Spanish [ðŸ‡ªðŸ‡¸]
German Native: [X]
German Formal: [X] 
Word-by-word Audio: [X]
Neural Networks: [X]
Cultural Adaptation: [X]
```

## Expected Output

### German Native
**Full sentence (native):**
â€žAnanassaft fÃ¼r das MÃ¤dchen und Brombeersaft fÃ¼r die Dame, weil sie im Krankenhaus sind und drauÃŸen regnet es."

**Word by word (with audio):**
â€¢ Ananassaft â†’ jugo de piÃ±a (Saft = jugo, Ananas = piÃ±a)
â€¢ fÃ¼r â†’ para
â€¢ das MÃ¤dchen â†’ la niÃ±a (MÃ¤dchen = niÃ±a)
â€¢ und â†’ y
â€¢ Brombeersaft â†’ jugo de mora (Brombeere = mora, Saft = jugo)
â€¢ fÃ¼r â†’ para
â€¢ die Dame â†’ la seÃ±ora (Dame = dama, seÃ±ora)
â€¢ weil â†’ porque
â€¢ sie â†’ ellos/ellas
â€¢ im Krankenhaus â†’ en el hospital (Krankenhaus = hospital, krank = enfermo, Haus = casa)
â€¢ sind â†’ estÃ¡n (del verbo sein = ser/estar)
â€¢ und â†’ y
â€¢ drauÃŸen â†’ afuera (literalmente: en el exterior)
â€¢ regnet es â†’ estÃ¡ lloviendo (regnet = llueve, es = ello)

---

### German Formal
**Full sentence (formal):**
â€žAnanassaft fÃ¼r das Kind und Brombeersaft fÃ¼r die verehrte Frau, da sie sich im Krankenhaus befinden und es drauÃŸen regnet."

**Word by word (with audio):**
â€¢ Ananassaft â†’ jugo de piÃ±a
â€¢ fÃ¼r â†’ para
â€¢ das Kind â†’ el/la niÃ±o/a (Kind = niÃ±o)
â€¢ und â†’ y
â€¢ Brombeersaft â†’ jugo de mora
â€¢ fÃ¼r â†’ para
â€¢ die verehrte Frau â†’ la seÃ±ora (verehrte = respetada/honorable, Frau = mujer, seÃ±ora)
â€¢ da â†’ porque (formal, mÃ¡s elegante que weil)
â€¢ sie sich â†’ ellos/ellas se
â€¢ im Krankenhaus â†’ en el hospital
â€¢ befinden â†’ se encuentran (verbo formal para "estar ubicados")
â€¢ und â†’ y
â€¢ es â†’ ello
â€¢ drauÃŸen â†’ afuera
â€¢ regnet â†’ llueve

## Neural Architecture Features

### 1. Transformer Encoder with CNN and BiLSTM
```python
# Text â†’ Character Recognition (CNN) â†’ BiLSTM â†’ Transformer Encoding
embedded = self.embedding(x) * math.sqrt(self.d_model)
cnn_output = F.relu(self.char_cnn(embedded.transpose(1, 2)))
lstm_output, _ = self.bilstm(combined)
transformer_output = self.transformer(lstm_output)
```

### 2. Multi-Head Attention for Context
```python
# Looking both forward and backward for context
scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
attention_weights = F.softmax(scores, dim=-1)
context = torch.matmul(attention_weights, V)
```

### 3. Decoder with Stacked LSTMs
```python
# Stacked LSTM-RNNs as requested in requirements
for lstm in self.stacked_lstms:
    lstm_output, _ = lstm(lstm_output)
    lstm_output = F.dropout(lstm_output, 0.1, self.training)
```

## Cultural Context Integration

The system handles:
- **Phrasal verbs**: "regnet es" â†’ "estÃ¡ lloviendo" (German verb-subject inversion)
- **Separable verbs**: Compound German verbs treated as semantic units
- **Cultural nuances**: "Dame" vs "Frau" for formality levels
- **Regional variations**: German formal "da" vs colloquial "weil"

## Technical Implementation

### Word Vectorization (Billions of Examples)
```python
# Uses pre-trained models with billions of examples
self.xlm_roberta_model = XLMRobertaModel.from_pretrained('xlm-roberta-base')
self.sentence_transformer = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

# Creates contextual word vectors
word_vectors = await self.create_word_vectors(text, language)
sentence_embedding = self.sentence_model.encode([text])[0]
```

### Statistical + Neural Hybrid
```python
# Combines Neural Machine Translation with Statistical Machine Translation
neural_translation = await self.translate_with_neural_network(...)
if neural_fails:
    fallback_translation = await self.smt_translate(...)
```

### Perfect UI-Audio Synchronization
- **Exact Format Match**: What you see = What you hear
- **Semantic Grouping**: Compound words stay together in audio
- **Context Preservation**: Explanations maintain meaning relationships

## API Usage

### Request Example
```json
{
  "text": "Jugo de piÃ±a para la niÃ±a y jugo de mora para la seÃ±ora porque estÃ¡n en el hospital y afuera estÃ¡ lloviendo.",
  "source_lang": "spanish",
  "target_lang": "multi",
  "style_preferences": {
    "german_native": true,
    "german_formal": true,
    "german_word_by_word": true,
    "use_neural_networks": true,
    "enable_cultural_adaptation": true,
    "mother_tongue": "spanish"
  }
}
```

### Response Example
```json
{
  "original_text": "Jugo de piÃ±a...",
  "translated_text": "Ananassaft fÃ¼r das MÃ¤dchen...",
  "translations": {
    "german_native": "â€žAnanassaft fÃ¼r das MÃ¤dchen..."",
    "german_formal": "â€žAnanassaft fÃ¼r das Kind...""
  },
  "word_by_word": {
    "ananassaft": {
      "spanish": "jugo de piÃ±a",
      "explanation": "Saft = jugo, Ananas = piÃ±a"
    }
  },
  "neural_models_used": ["transformer", "encoder-decoder", "bidirectional-rnn", "attention"],
  "semantic_analysis": {
    "cultural_markers": ["hospital", "seÃ±ora"],
    "morphological_complexity": 2.3
  }
}
```

## Settings Screen Integration

The Flutter UI provides complete control over:

**Neural Translation Engine:**
- âœ… Attention Visualization
- âœ… Cultural Adaptation  
- âœ… Semantic Consistency
- âœ… Formality Preservation

**Language Styles:**
- ðŸ‡©ðŸ‡ª German: Native, Colloquial, Informal, Formal
- ðŸ‡ºðŸ‡¸ English: Native, Colloquial, Informal, Formal
- ðŸŽµ Word-by-word Audio for both languages

**Advanced Options:**
- Temperature: 0.1-1.5 (creativity control)
- Context Window: 128-2048 tokens
- Top-K: 10-100 (word selection)
- Top-P: 0.5-1.0 (probability threshold)

## Performance Optimizations

1. **Model Caching**: Pre-trained models loaded once
2. **Translation Cache**: Frequent phrases cached
3. **Batch Processing**: Multiple requests processed together  
4. **Attention Optimization**: Efficient matrix operations
5. **Memory Management**: Smart cleanup of embeddings

## Supported Language Combinations

Based on mother tongue selection:
- **Spanish** â†’ German (Native/Formal/Informal/Colloquial) + English
- **English** â†’ German + Spanish (automatic)
- **German** â†’ English + Spanish (automatic)

All combinations support word-by-word audio with perfect synchronization.