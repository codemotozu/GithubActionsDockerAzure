# translation_service.py - Complete file with multi-style support

from google.generativeai import GenerativeModel
import google.generativeai as genai
import os
from dotenv import load_dotenv
from ...domain.entities.translation import Translation
from spellchecker import SpellChecker
import unicodedata
import regex as re
from .tts_service import EnhancedTTSService
from .universal_ai_translation_service import universal_ai_translator
import tempfile
from typing import Optional, Dict, List, Tuple
import asyncio
import json


class TranslationService:
    def __init__(self):
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found in environment variables")

        genai.configure(api_key=api_key)

        self.spell = SpellChecker()

        self.generation_config = {
            "temperature": 0.3,  # Lower for more consistent translations
            "top_p": 0.8,
            "top_k": 40,
            "max_output_tokens": 8192,
        }

        self.model = GenerativeModel(
            model_name="gemini-2.0-flash", generation_config=self.generation_config
        )

        self.tts_service = EnhancedTTSService()

        # Audio generation settings
        self.audio_retry_attempts = 2
        self.audio_timeout_seconds = 30

        # Language detection patterns for dynamic mother tongue detection
        self.language_patterns = {
            'spanish': {
                'keywords': ['yo', 't√∫', '√©l', 'ella', 'nosotros', 'vosotros', 'ellos', 'ellas', 'es', 'son', 'est√°', 'est√°n', 'tiene', 'tienes', 'para', 'con', 'de', 'en', 'la', 'el', 'los', 'las', 'un', 'una', 'unos', 'unas'],
                'patterns': [r'\b(soy|eres|somos|sois|son)\b', r'\b(tengo|tienes|tiene|tenemos|ten√©is|tienen)\b', r'\b(estoy|est√°s|est√°|estamos|est√°is|est√°n)\b']
            },
            'english': {
                'keywords': ['i', 'you', 'he', 'she', 'we', 'they', 'am', 'is', 'are', 'was', 'were', 'have', 'has', 'had', 'the', 'a', 'an', 'and', 'or', 'but', 'with', 'for', 'to'],
                'patterns': [r'\b(am|is|are|was|were)\b', r'\b(have|has|had)\b', r'\b(do|does|did)\b']
            },
            'german': {
                'keywords': ['ich', 'du', 'er', 'sie', 'wir', 'ihr', 'sie', 'bin', 'bist', 'ist', 'sind', 'war', 'waren', 'der', 'die', 'das', 'den', 'dem', 'des', 'ein', 'eine', 'einen', 'einem', 'einer', 'und', 'oder', 'aber', 'mit', 'f√ºr', 'zu'],
                'patterns': [r'\b(bin|bist|ist|sind|war|waren)\b', r'\b(habe|hast|hat|haben|hatte|hatten)\b', r'\b(mache|machst|macht|machen)\b']
            }
        }

    def _detect_input_language(self, text: str, mother_tongue: str = None) -> str:
        """Detect the language of input text. If mother_tongue is provided, use it as primary hint."""
        text_lower = text.lower()
        
        # If mother tongue is explicitly provided, prioritize it
        if mother_tongue:
            return mother_tongue
            
        # Score each language based on keyword matches
        scores = {'spanish': 0, 'english': 0, 'german': 0}
        
        for lang, data in self.language_patterns.items():
            # Count keyword matches
            for keyword in data['keywords']:
                if f' {keyword} ' in f' {text_lower} ' or text_lower.startswith(f'{keyword} ') or text_lower.endswith(f' {keyword}'):
                    scores[lang] += 1
            
            # Count pattern matches
            for pattern in data['patterns']:
                matches = re.findall(pattern, text_lower)
                scores[lang] += len(matches) * 2  # Patterns are weighted higher
        
        # Return the language with highest score, default to spanish if tied
        detected = max(scores, key=scores.get)
        print(f"üîç Language detection scores: {scores} -> Detected: {detected}")
        return detected


    def _create_enhanced_context_prompt(self, input_text: str, mother_tongue: str, style_preferences) -> str:
        """Create enhanced prompt for multiple simultaneous styles with true semantic matching."""
        
        print(f"üéØ Creating MULTI-STYLE context prompt for: {mother_tongue.upper()}")
        
        target_languages = []
        german_styles = []
        english_styles = []
        
        # Collect all selected German styles
        if style_preferences.german_native:
            german_styles.append('native')
        if style_preferences.german_colloquial:
            german_styles.append('colloquial')
        if style_preferences.german_informal:
            german_styles.append('informal')
        if style_preferences.german_formal:
            german_styles.append('formal')
        
        # Collect all selected English styles
        if style_preferences.english_native:
            english_styles.append('native')
        if style_preferences.english_colloquial:
            english_styles.append('colloquial')
        if style_preferences.english_informal:
            english_styles.append('informal')
        if style_preferences.english_formal:
            english_styles.append('formal')
        
        # Determine target languages based on mother tongue and selections
        if mother_tongue.lower() == 'spanish':
            if german_styles:
                target_languages.append('german')
            if english_styles:
                target_languages.append('english')
        elif mother_tongue.lower() == 'english':
            target_languages.append('spanish')
            if german_styles:
                target_languages.append('german')
        elif mother_tongue.lower() == 'german':
            target_languages.append('spanish')
            if english_styles:
                target_languages.append('english')

        print(f"üéØ Target languages: {target_languages}")
        print(f"üá©üá™ German styles selected: {german_styles}")
        print(f"üá∫üá∏ English styles selected: {english_styles}")

        # Build comprehensive prompt with advanced semantic matching guidance
        prompt = f"""Translate the {mother_tongue} text: "{input_text}"

    You are an expert linguist specializing in semantic translation between {mother_tongue} and {', '.join(target_languages)}. 
    Please provide ALL requested translations in this EXACT format:

    """

        # Add German translations for ALL selected styles
        if 'german' in target_languages and german_styles:
            prompt += "GERMAN TRANSLATIONS:\n"
            for style in german_styles:
                prompt += f"German {style.capitalize()}: [Provide {style} German translation here]\n"
            
            # Add word-by-word section with EXACT semantic matching
            if style_preferences.german_word_by_word:
                prompt += "\nGERMAN WORD-BY-WORD:\n"
                for style in german_styles:
                    prompt += f"{style.capitalize()} style: "
                    prompt += f"CRITICALLY IMPORTANT: Provide EXACT semantic word-by-word mappings for the Spanish input '{input_text}'. "
                    prompt += "Format: [German_word] ([Spanish_equivalent_from_original_text]). "
                    prompt += f"\n\nFor '{input_text}', your German translation word-by-word mapping MUST follow these EXACT rules:\n"
                    prompt += "1. Each German word maps to its SEMANTIC EQUIVALENT from the original Spanish text\n"
                    prompt += "2. Compound German words (like 'Ananassaft') map to compound Spanish phrases (like 'jugo de pi√±a')\n"
                    prompt += "3. German articles (das, die, der) map to Spanish articles (la, el, las, los)\n"
                    prompt += "4. German prepositions map to their Spanish equivalents: f√ºr=para, von=de, mit=con\n"
                    prompt += "5. German nouns map to their Spanish equivalents: M√§dchen=ni√±a, Dame=se√±ora\n\n"
                    prompt += f"EXAMPLE for '{input_text}':\n"
                    prompt += "If German translation is 'Ananassaft f√ºr das M√§dchen und Brombeersaft f√ºr die Dame', then:\n"
                    prompt += "[Ananassaft] ([jugo de pi√±a]) [f√ºr] ([para]) [das] ([la]) [M√§dchen] ([ni√±a]) [und] ([y]) [Brombeersaft] ([jugo de mora]) [f√ºr] ([para]) [die] ([la]) [Dame] ([se√±ora])\n\n"
                    prompt += "CRITICAL: Each mapping MUST be semantically correct:\n"
                    prompt += "- Ananassaft = jugo de pi√±a (pineapple juice)\n"
                    prompt += "- f√ºr = para (for)\n" 
                    prompt += "- das = la (the, feminine)\n"
                    prompt += "- M√§dchen = ni√±a (girl)\n"
                    prompt += "- und = y (and)\n"
                    prompt += "- Brombeersaft = jugo de mora (blackberry juice)\n"
                    prompt += "- die = la (the, feminine)\n"
                    prompt += "- Dame = se√±ora (lady)\n\n"
                prompt += "\n"

        # Add English translations for ALL selected styles
        if 'english' in target_languages and english_styles:
            prompt += "ENGLISH TRANSLATIONS:\n"
            for style in english_styles:
                prompt += f"English {style.capitalize()}: [Provide {style} English translation here]\n"
            
            # Add word-by-word section with EXACT semantic matching
            if style_preferences.english_word_by_word:
                prompt += "\nENGLISH WORD-BY-WORD:\n"
                for style in english_styles:
                    prompt += f"{style.capitalize()} style: "
                    prompt += f"CRITICALLY IMPORTANT: Provide EXACT semantic word-by-word mappings for the Spanish input '{input_text}'. "
                    prompt += "Format: [English_word] ([Spanish_equivalent_from_original_text]). "
                    prompt += f"\n\nFor '{input_text}', your English translation word-by-word mapping MUST follow these EXACT rules:\n"
                    prompt += "1. Each English word maps to its SEMANTIC EQUIVALENT from the original Spanish text\n"
                    prompt += "2. Compound English phrases (like 'pineapple juice') map to compound Spanish phrases (like 'jugo de pi√±a')\n"
                    prompt += "3. English articles (the, a, an) map to Spanish articles (la, el, las, los, un, una)\n"
                    prompt += "4. English prepositions map to their Spanish equivalents: for=para, of=de, with=con\n"
                    prompt += "5. English nouns map to their Spanish equivalents: girl=ni√±a, lady=se√±ora\n\n"
                    prompt += f"EXAMPLE for '{input_text}':\n"
                    prompt += "If English translation is 'Pineapple juice for the girl and blackberry juice for the lady', then:\n"
                    prompt += "[Pineapple juice] ([jugo de pi√±a]) [for] ([para]) [the] ([la]) [girl] ([ni√±a]) [and] ([y]) [blackberry juice] ([jugo de mora]) [for] ([para]) [the] ([la]) [lady] ([se√±ora])\n\n"
                    prompt += "CRITICAL: Each mapping MUST be semantically correct:\n"
                    prompt += "- Pineapple juice = jugo de pi√±a (whole phrase)\n"
                    prompt += "- for = para (for)\n" 
                    prompt += "- the = la (the, feminine)\n"
                    prompt += "- girl = ni√±a (girl)\n"
                    prompt += "- and = y (and)\n"
                    prompt += "- blackberry juice = jugo de mora (whole phrase)\n"
                    prompt += "- lady = se√±ora (lady)\n\n"
                prompt += "\n"

        # Add Spanish translations if needed
        if 'spanish' in target_languages:
            prompt += "SPANISH TRANSLATIONS:\nSpanish Colloquial: [Spanish translation here]\n\n"

        # Add CRITICAL final instructions
        prompt += f"""
ABSOLUTELY CRITICAL FINAL INSTRUCTIONS:

The Spanish input is: '{input_text}'

You MUST create word-by-word mappings that follow these EXACT semantic principles:

1. SEMANTIC EQUIVALENCE: Each word maps to what it MEANS, not its position
2. COMPOUND WORDS: Handle appropriately
   - German "Ananassaft" = Spanish "jugo de pi√±a" (as one mapping)
   - English "Pineapple juice" = Spanish "jugo de pi√±a" (as one mapping)
3. EXACT WORD USAGE: Only use words that appear in '{input_text}'
4. ARTICLES: Map correctly (das/die/der ‚Üí la/el/las/los, the ‚Üí la/el/las/los)
5. PREPOSITIONS: Map correctly (f√ºr ‚Üí para, for ‚Üí para, NOT "de"!)

WRONG EXAMPLE (DO NOT DO THIS):
[das] ([pi√±a]) [M√§dchen] ([para]) [f√ºr] ([de])

CORRECT EXAMPLE (DO THIS):
[das] ([la]) [M√§dchen] ([ni√±a]) [f√ºr] ([para])

Your word-by-word mappings will be used for language learning audio. Students need to hear CORRECT semantic translations. Any errors will confuse learners."""

        print(f"üìù Generated MULTI-STYLE prompt ({len(prompt)} characters)")
        return prompt



    def _should_generate_audio(self, translations_data: Dict, style_preferences) -> bool:
        """Only generate audio if user has selected translation styles"""
        has_translations = len(translations_data.get('translations', [])) > 0
        
        has_enabled_styles = False
        if style_preferences:
            # Check ALL style preferences
            style_checks = [
                style_preferences.german_native,
                style_preferences.german_colloquial,
                style_preferences.german_informal,
                style_preferences.german_formal,
                style_preferences.english_native,
                style_preferences.english_colloquial,
                style_preferences.english_informal,
                style_preferences.english_formal
            ]
            has_enabled_styles = any(style_checks)
        
        word_by_word_requested = False
        if style_preferences:
            word_by_word_requested = (
                style_preferences.german_word_by_word or 
                style_preferences.english_word_by_word
            )
        
        should_generate = has_translations and has_enabled_styles
        
        print(f"üéµ Audio Generation Decision:")
        print(f"   Translations available: {has_translations}")
        print(f"   Translation styles enabled: {has_enabled_styles}")
        print(f"   Word-by-word audio requested: {word_by_word_requested}")
        print(f"   üéØ Will generate audio: {should_generate}")
        print(f"   üéØ Audio type: {'Word-by-word breakdown' if word_by_word_requested else 'Simple translation reading'}")
        
        return should_generate

    async def _generate_audio_with_resilience(self, translations_data: Dict, detected_mother_tongue: str, style_preferences) -> Optional[str]:
        """Generate audio with enhanced error handling for multiple styles"""
        try:
            print("üéµ Attempting MULTI-STYLE SYNCHRONIZED audio generation...")
            
            audio_task = asyncio.create_task(
                self.tts_service.text_to_speech_word_pairs_v2(
                    translations_data=translations_data,
                    source_lang=detected_mother_tongue,
                    target_lang="es",
                    style_preferences=style_preferences,
                )
            )
            
            try:
                audio_filename = await asyncio.wait_for(audio_task, timeout=self.audio_timeout_seconds)
                
                if audio_filename:
                    print(f"‚úÖ MULTI-STYLE SYNCHRONIZED audio generation successful: {audio_filename}")
                    return audio_filename
                else:
                    print("‚ö†Ô∏è Audio generation returned None")
                    return None
                    
            except asyncio.TimeoutError:
                print(f"‚è∞ Audio generation timed out after {self.audio_timeout_seconds} seconds")
                audio_task.cancel()
                return None
                
        except Exception as e:
            print(f"‚ùå Audio generation failed: {str(e)}")
            return None


    # Update the process_prompt method to include the original text in the translations data
    async def process_prompt(
        self, text: str, source_lang: str, target_lang: str, style_preferences=None, mother_tongue: str = None
    ) -> Translation:
        try:
            # Use default preferences if none provided
            if style_preferences is None:
                from ...infrastructure.api.routes import TranslationStylePreferences
                style_preferences = TranslationStylePreferences(
                    german_colloquial=True,
                    english_colloquial=True,
                    mother_tongue="spanish"
                )

            # Detect the actual input language
            detected_mother_tongue = self._detect_input_language(text, mother_tongue)
            
            print(f"\n{'='*80}")
            print(f"üåê PROCESSING MULTI-STYLE ENHANCED CONTEXTUAL TRANSLATION")
            print(f"{'='*80}")
            print(f"üìù Input text: '{text}'")
            print(f"üåç Detected mother tongue: {detected_mother_tongue}")

            # Create enhanced multi-style context prompt 
            enhanced_prompt = self._create_enhanced_context_prompt(
                text, detected_mother_tongue, style_preferences
            )
            
            print(f"üì§ Sending MULTI-STYLE prompt to Gemini AI...")

            try:
                # Use direct model call for more reliable parsing
                response = self.model.generate_content(enhanced_prompt)
                generated_text = response.text

                print(f"üì• Gemini response received ({len(generated_text)} characters)")
                print(f"üìÑ Response preview: {generated_text[:200]}...")
                print(f"üîç DEBUG - Full AI response:")
                print("=" * 80)
                print(generated_text)
                print("=" * 80)

            except Exception as e:
                print(f"‚ùå Gemini API error: {str(e)}")
                # Fallback response
                generated_text = f"Translation error for '{text}'. Please try again."
                translations_data = {'translations': [generated_text], 'style_data': [], 'original_text': text}
                
                return Translation(
                    original_text=text,
                    translated_text=generated_text,
                    source_language=detected_mother_tongue,
                    target_language="multi",
                    audio_path=None,
                    translations={"main": generated_text},
                    word_by_word=None,
                    grammar_explanations=None,
                )

            # Extract translations with MULTI-STYLE support
            translations_data = await self._extract_translations_fixed(generated_text, style_preferences)
            
            # Store original text for fallback word-by-word generation
            translations_data['original_text'] = text

            audio_filename = None

            # Check if audio should be generated
            should_generate_audio = self._should_generate_audio(translations_data, style_preferences)
            
            # Generate synchronized audio for all selected styles
            if should_generate_audio:
                print("üéµ Starting MULTI-STYLE SYNCHRONIZED audio generation...")
                audio_filename = await self._generate_audio_with_resilience(
                    translations_data, detected_mother_tongue, style_preferences
                )
                
                if audio_filename:
                    print(f"‚úÖ MULTI-STYLE SYNCHRONIZED audio completed: {audio_filename}")
                else:
                    print("‚ÑπÔ∏è Audio generation failed/skipped - continuing without audio")
            else:
                print("üîá No audio generated - no translation styles enabled")

            # Create perfect UI-Audio synchronized data for all styles
            ui_word_by_word = self._create_perfect_ui_sync_data(translations_data, style_preferences)

            # Create final translation text with all styles
            final_translation_text = self._create_formatted_translation_text(translations_data)

            # Create AI-powered structured styles data for frontend
            styles_data = await self._create_styles_data(translations_data)

            # Create comprehensive translations map for all styles
            all_translations = {
                "main": translations_data['translations'][0] if translations_data['translations'] else final_translation_text
            }
            
            # Add each style's translation to the translations map
            for style_info in translations_data.get('style_data', []):
                style_name = style_info.get('style_name', '')
                translation_text = style_info.get('translation', '')
                if style_name and translation_text:
                    all_translations[style_name] = translation_text

            return Translation(
                original_text=text,
                translated_text=final_translation_text,
                source_language=detected_mother_tongue,
                target_language="multi",
                audio_path=audio_filename if audio_filename else None,
                translations=all_translations,  # Now includes all styles
                word_by_word=ui_word_by_word,  # CRITICAL: Preserve for audio sync
                grammar_explanations=self._generate_grammar_explanations(final_translation_text),
                styles=styles_data,  # CRITICAL: Complete translations for display
            )

        except Exception as e:
            print(f"‚ùå Error in process_prompt: {str(e)}")
            import traceback
            traceback.print_exc()
            raise Exception(f"Translation processing failed: {str(e)}")

    async def _fix_common_semantic_mismatches(self, pairs: List[Tuple[str, str]], is_german: bool = True) -> List[Tuple[str, str]]:
        """
        Apply AI-powered semantic corrections instead of static dictionaries.
        This uses artificial intelligence to handle billions of possible word combinations.
        """
        try:
            # Import AI semantic corrector
            from .ai_semantic_corrector import ai_semantic_corrector
            
            # Determine source and target languages
            source_language = "German" if is_german else "English"  
            target_language = "Spanish"
            
            print(f"ü§ñ Using AI semantic correction for {source_language} ‚Üí {target_language}")
            print(f"üß† Processing {len(pairs)} word pairs with artificial intelligence...")
            
            # Use AI to detect and correct semantic mismatches
            semantic_analysis = await ai_semantic_corrector.correct_semantic_mismatches(
                word_pairs=pairs,
                source_language=source_language,
                target_language=target_language
            )
            
            # Apply AI corrections
            corrected_pairs = []
            corrections_made = 0
            
            # Create correction lookup from AI analysis
            correction_lookup = {}
            for correction in semantic_analysis.corrections:
                key = (correction.original_source, correction.original_target)
                correction_lookup[key] = correction.corrected_target
                corrections_made += 1
                
                print(f"üéµ {correction.original_source} ‚Üí {correction.original_target} corrected to {correction.corrected_target} (confidence: {correction.confidence:.2f})")
                print(f"   Reason: {correction.correction_reason}")
                print(f"   Category: {correction.linguistic_category}")
            
            # Apply corrections to original pairs
            for source, target in pairs:
                if (source, target) in correction_lookup:
                    corrected_target = correction_lookup[(source, target)]
                    corrected_pairs.append((source, corrected_target))
                else:
                    corrected_pairs.append((source, target))
            
            print(f"‚úÖ AI semantic analysis completed:")
            print(f"   - {corrections_made} corrections applied")
            print(f"   - Overall accuracy: {semantic_analysis.overall_accuracy:.2f}")
            print(f"   - AI confidence: {semantic_analysis.ai_confidence:.2f}")
            print(f"   - Processing time: {semantic_analysis.processing_time:.2f}s")
            
            return corrected_pairs
            
        except Exception as e:
            print(f"‚ö†Ô∏è AI semantic correction failed, using fallback: {e}")
            
            # Fallback: minimal critical corrections only
            corrected_pairs = []
            corrections_made = 0
            
            for source, target in pairs:
                # Only handle the most critical semantic mismatches as fallback
                corrected_target = target
                source_lower = source.lower()
                target_lower = target.lower()
                
                # Critical fallback corrections (minimal set)
                if is_german:
                    if source_lower == 'das' and target_lower == 'la':
                        corrected_target = 'lo'  # Neuter article correction
                        corrections_made += 1
                    elif source_lower == 'ich bin' and 'levanto' in target_lower:
                        corrected_target = 'yo soy'  # "I am" correction
                        corrections_made += 1
                    elif source_lower == 'f√ºr' and target_lower != 'para':
                        corrected_target = 'para'  # "for" correction
                        corrections_made += 1
                else:
                    # English fallbacks
                    if 'wake up' in source_lower and 'despertar' in target_lower and 'se' not in target_lower:
                        corrected_target = 'despertarse'  # Reflexive verb correction
                        corrections_made += 1
                
                corrected_pairs.append((source, corrected_target))
            
            if corrections_made > 0:
                print(f"üîß Applied {corrections_made} fallback semantic corrections")
            
            return corrected_pairs


    def _is_likely_plural(self, pairs: List[Tuple[str, str]]) -> bool:
        """
        Detect if the sentence context is likely referring to plural entities.
        """
        # Check for typical plural indicators
        for source, _ in pairs:
            source_lower = source.lower()
            # Common plural nouns endings in English and German
            if source_lower.endswith('s') or source_lower.endswith('en'):
                if source_lower in ['hands', 'fingers', 'arms', 'legs', 'feet', 
                                'h√§nde', 'finger', 'arme', 'beine', 'f√º√üe']:
                    return True
        
        return False


    def _ensure_word_by_word_data(self, translations_data: Dict, style_preferences) -> Dict:
        """
        Ensure word-by-word data is available when requested.
        If the AI failed to provide word-by-word data, generate fallback data.
        """
        # Check if word-by-word is requested for any language
        german_word_by_word = getattr(style_preferences, 'german_word_by_word', False)
        english_word_by_word = getattr(style_preferences, 'english_word_by_word', False)
        any_word_by_word_requested = german_word_by_word or english_word_by_word
        
        if not any_word_by_word_requested:
            # No word-by-word requested, no need to check or generate
            return translations_data
        
        # Check if we have any styles with word-by-word data
        has_word_by_word_data = False
        for style_info in translations_data.get('style_data', []):
            if style_info.get('word_pairs', []):
                has_word_by_word_data = True
                break
        
        # If we have word-by-word data, no need to generate fallback
        if has_word_by_word_data:
            return translations_data
        
        print("‚ö†Ô∏è Word-by-word audio requested but no data found in AI response - generating fallback data")
        
        # Generate fallback word-by-word data for each style that needs it
        for style_info in translations_data.get('style_data', []):
            translation_text = style_info.get('translation', '')
            is_german = style_info.get('is_german', False)
            is_spanish = style_info.get('is_spanish', False)
            style_name = style_info.get('style_name', 'unknown')
            
            # Check if this style should have word-by-word data
            should_include = False
            if is_german and german_word_by_word:
                should_include = True
            elif not is_german and not is_spanish and english_word_by_word:
                should_include = True
            
            if should_include and translation_text:
                # Generate basic word-by-word data by splitting the translation
                words = translation_text.split()
                
                # For Spanish mother tongue, use original text to create pairs
                original_words = translations_data.get('original_text', '').split()
                
                # Generate pairs from translation text
                fallback_pairs = []
                
                # If we have original words (mother tongue), try to match proportionally
                if original_words:
                    # Simple approach: align words proportionally
                    for i, word in enumerate(words):
                        # Calculate corresponding index in original words
                        original_idx = min(int(i * len(original_words) / len(words)), len(original_words) - 1)
                        spanish_equiv = original_words[original_idx] if original_idx < len(original_words) else ""
                        
                        fallback_pairs.append((word, spanish_equiv))
                else:
                    # No original words available, just use the translation with empty Spanish equivalents
                    fallback_pairs = [(word, "") for word in words]
                
                print(f"‚úÖ Generated {len(fallback_pairs)} fallback word pairs for {style_name}")
                style_info['word_pairs'] = fallback_pairs
        
        return translations_data

    def _ensure_complete_translations(self, translations_data: Dict, style_preferences, generated_text: str) -> Dict:
        """
        Ensure we have complete sentence translations for all enabled styles.
        If any style is missing its complete translation, extract it from the generated text.
        """
        print("üîç Ensuring complete translations for all enabled styles...")
        
        # Define all possible styles
        all_styles = {
            'german_native': style_preferences.german_native,
            'german_colloquial': style_preferences.german_colloquial,
            'german_informal': style_preferences.german_informal,
            'german_formal': style_preferences.german_formal,
            'english_native': style_preferences.english_native,
            'english_colloquial': style_preferences.english_colloquial,
            'english_informal': style_preferences.english_informal,
            'english_formal': style_preferences.english_formal
        }
        
        # Find which styles are enabled but missing from style_data
        existing_styles = {style_info['style_name'] for style_info in translations_data.get('style_data', [])}
        
        for style_name, is_enabled in all_styles.items():
            if is_enabled and style_name not in existing_styles:
                print(f"‚ö†Ô∏è Missing translation for enabled style: {style_name}")
                
                # Try to extract the translation from the generated text
                complete_translation = self._extract_style_translation_from_full_text(generated_text, style_name)
                
                if complete_translation:
                    print(f"‚úÖ Successfully extracted complete translation for {style_name}: {complete_translation[:50]}...")
                    
                    # Add to translations_data
                    translations_data['translations'].append(complete_translation)
                    translations_data['style_data'].append({
                        'translation': complete_translation,
                        'word_pairs': [],
                        'is_german': 'german' in style_name,
                        'is_spanish': False,
                        'style_name': style_name
                    })
                else:
                    # Generate a fallback translation
                    fallback_translation = self._generate_fallback_translation(style_name, translations_data.get('original_text', ''))
                    print(f"üìù Generated fallback translation for {style_name}: {fallback_translation}")
                    
                    translations_data['translations'].append(fallback_translation)
                    translations_data['style_data'].append({
                        'translation': fallback_translation,
                        'word_pairs': [],
                        'is_german': 'german' in style_name,
                        'is_spanish': False,
                        'style_name': style_name
                    })
        
        return translations_data
    
    def _extract_style_translation_from_full_text(self, generated_text: str, style_name: str) -> Optional[str]:
        """Extract a specific style's complete translation from the full generated text"""
        lines = generated_text.split('\n')
        
        # Create patterns to look for
        style_patterns = {
            'german_native': ['German Native:', 'Native:'],
            'german_colloquial': ['German Colloquial:', 'Colloquial:'],
            'german_informal': ['German Informal:', 'Informal:'],
            'german_formal': ['German Formal:', 'Formal:'],
            'english_native': ['English Native:', 'Native:'],
            'english_colloquial': ['English Colloquial:', 'Colloquial:'],
            'english_informal': ['English Informal:', 'Informal:'],
            'english_formal': ['English Formal:', 'Formal:']
        }
        
        patterns = style_patterns.get(style_name, [])
        
        for i, line in enumerate(lines):
            for pattern in patterns:
                if pattern in line:
                    # Try to extract from this line first
                    translation = self._extract_translation_from_line(line, pattern)
                    if translation:
                        return translation
                    
                    # If not found in current line, check next few lines
                    for j in range(i + 1, min(i + 4, len(lines))):
                        next_line = lines[j].strip()
                        if next_line and not next_line.startswith(('German', 'English', 'GERMAN', 'ENGLISH', '*', '-')):
                            # This might be our translation
                            clean_translation = next_line.strip('[]"\'').strip()
                            if len(clean_translation) > 3 and 'translation here' not in clean_translation.lower():
                                return clean_translation
        
        return None
    
    def _generate_fallback_translation(self, style_name: str, original_text: str) -> str:
        """Generate a fallback translation when AI extraction completely fails"""
        language = 'German' if 'german' in style_name else 'English'
        style = style_name.split('_')[1].title() if '_' in style_name else 'Standard'
        
        if original_text:
            return f"Complete {language} {style} translation of: '{original_text}'"
        else:
            return f"{language} {style} translation - Processing complete sentence"

    async def _extract_translations_fixed(self, generated_text: str, style_preferences) -> Dict:
        """Enhanced extraction for multiple simultaneous styles with semantic correction and fallback."""
        result = {
            'translations': [],
            'style_data': [],
            'original_text': ""  # Store original text for fallback generation
        }

        print("üîç EXTRACTING MULTI-STYLE TRANSLATIONS")
        print("="*50)
        print(f"üìÑ Generated text length: {len(generated_text)}")

        try:
            lines = generated_text.split('\n')
            current_language = None
            word_by_word_text = {}
            all_word_by_word_data = {}  # Store word-by-word for ALL styles
            
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                
                # Detect language sections
                if 'GERMAN TRANSLATIONS:' in line.upper():
                    current_language = 'german'
                    print(f"üìç Found German section")
                elif 'ENGLISH TRANSLATIONS:' in line.upper():
                    current_language = 'english'
                    print(f"üìç Found English section")
                elif 'SPANISH TRANSLATIONS:' in line.upper():
                    current_language = 'spanish'
                    print(f"üìç Found Spanish section")
                elif 'GERMAN WORD-BY-WORD:' in line.upper():
                    print(f"üìç Found German word-by-word section")
                    current_language = 'german_wbw'
                elif 'ENGLISH WORD-BY-WORD:' in line.upper():
                    print(f"üìç Found English word-by-word section")
                    current_language = 'english_wbw'
                
                # Extract translations for ALL selected styles
                elif current_language == 'german':
                    # Process ALL German styles
                    styles_to_check = [
                        ('German Native:', 'german_native', style_preferences.german_native),
                        ('German Colloquial:', 'german_colloquial', style_preferences.german_colloquial),
                        ('German Informal:', 'german_informal', style_preferences.german_informal),
                        ('German Formal:', 'german_formal', style_preferences.german_formal)
                    ]
                    
                    for prefix, style_name, is_enabled in styles_to_check:
                        if prefix in line and is_enabled:
                            translation = self._extract_translation_from_line(line, prefix)
                            if translation:
                                result['translations'].append(translation)
                                result['style_data'].append({
                                    'translation': translation,
                                    'word_pairs': [],
                                    'is_german': True,
                                    'is_spanish': False,
                                    'style_name': style_name
                                })
                                print(f"‚úÖ {style_name}: {translation[:50]}...")
                            else:
                                # Ensure we still add the style entry even if extraction failed
                                print(f"‚ö†Ô∏è Failed to extract translation for {style_name} from line: {line}")
                                result['style_data'].append({
                                    'translation': f'Translation for {style_name.replace("_", " ").title()}',
                                    'word_pairs': [],
                                    'is_german': True,
                                    'is_spanish': False,
                                    'style_name': style_name
                                })
                
                elif current_language == 'english':
                    # Process ALL English styles
                    styles_to_check = [
                        ('English Native:', 'english_native', style_preferences.english_native),
                        ('English Colloquial:', 'english_colloquial', style_preferences.english_colloquial),
                        ('English Informal:', 'english_informal', style_preferences.english_informal),
                        ('English Formal:', 'english_formal', style_preferences.english_formal)
                    ]
                    
                    for prefix, style_name, is_enabled in styles_to_check:
                        if prefix in line and is_enabled:
                            translation = self._extract_translation_from_line(line, prefix)
                            if translation:
                                result['translations'].append(translation)
                                result['style_data'].append({
                                    'translation': translation,
                                    'word_pairs': [],
                                    'is_german': False,
                                    'is_spanish': False,
                                    'style_name': style_name
                                })
                                print(f"‚úÖ {style_name}: {translation[:50]}...")
                            else:
                                # Ensure we still add the style entry even if extraction failed
                                print(f"‚ö†Ô∏è Failed to extract translation for {style_name} from line: {line}")
                                result['style_data'].append({
                                    'translation': f'Translation for {style_name.replace("_", " ").title()}',
                                    'word_pairs': [],
                                    'is_german': False,
                                    'is_spanish': False,
                                    'style_name': style_name
                                })
                
                # Handle word-by-word sections for multiple styles
                elif current_language == 'german_wbw':
                    # Check if this line specifies a style
                    for style in ['Native', 'Colloquial', 'Informal', 'Formal']:
                        if f'{style} style:' in line:
                            style_key = f'german_{style.lower()}'
                            # Extract the word-by-word part
                            wbw_start = line.find('[')
                            if wbw_start >= 0:
                                all_word_by_word_data[style_key] = line[wbw_start:]
                                print(f"üìù German {style} word-by-word: {line[wbw_start:200]}...")
                                print(f"üîç Full line for debugging: {line}")
                            break
                    else:
                        # If line contains brackets, might be general word-by-word
                        if '[' in line and ']' in line and '(' in line and ')' in line:
                            if 'german' not in word_by_word_text:
                                word_by_word_text['german'] = line
                                print(f"üìù German general word-by-word: {line[:100]}...")
                
                elif current_language == 'english_wbw':
                    # Check if this line specifies a style
                    for style in ['Native', 'Colloquial', 'Informal', 'Formal']:
                        if f'{style} style:' in line:
                            style_key = f'english_{style.lower()}'
                            # Extract the word-by-word part
                            wbw_start = line.find('[')
                            if wbw_start >= 0:
                                all_word_by_word_data[style_key] = line[wbw_start:]
                                print(f"üìù English {style} word-by-word: {line[wbw_start:200]}...")
                                print(f"üîç Full line for debugging: {line}")
                            break
                    else:
                        # If line contains brackets, might be general word-by-word
                        if '[' in line and ']' in line and '(' in line and ')' in line:
                            if 'english' not in word_by_word_text:
                                word_by_word_text['english'] = line
                                print(f"üìù English general word-by-word: {line[:100]}...")
                
                elif current_language == 'spanish':
                    if 'Spanish Colloquial:' in line:
                        translation = self._extract_translation_from_line(line, 'Spanish Colloquial:')
                        if translation:
                            result['translations'].append(translation)
                            result['style_data'].append({
                                'translation': translation,
                                'word_pairs': [],
                                'is_german': False,
                                'is_spanish': True,
                                'style_name': 'spanish_colloquial'
                            })
                            print(f"‚úÖ Spanish Colloquial: {translation[:50]}...")

            # Process word-by-word data for EACH style with semantic correction
            for style_entry in result['style_data']:
                style_name = style_entry['style_name']
                is_german = style_entry['is_german']
                
                # Check if we have specific word-by-word for this style
                if style_name in all_word_by_word_data:
                    # Use style-specific word-by-word
                    if (is_german and style_preferences.german_word_by_word) or \
                    (not is_german and not style_entry['is_spanish'] and style_preferences.english_word_by_word):
                        # Extract the initial word pairs
                        word_pairs = self._parse_word_by_word_line(all_word_by_word_data[style_name])
                        
                        if word_pairs:
                            # Apply semantic corrections
                            corrected_pairs = await self._fix_common_semantic_mismatches(
                                word_pairs, 
                                is_german=is_german
                            )
                            
                            # Store the corrected pairs
                            style_entry['word_pairs'] = corrected_pairs
                            print(f"‚úÖ Added {len(corrected_pairs)} semantically-corrected word pairs to {style_name}")
                else:
                    # Fall back to general word-by-word if available
                    language = 'german' if is_german else 'english'
                    if language in word_by_word_text:
                        if (is_german and style_preferences.german_word_by_word) or \
                        (not is_german and style_preferences.english_word_by_word):
                            # Extract the initial word pairs
                            word_pairs = self._parse_word_by_word_line(word_by_word_text[language])
                            
                            if word_pairs:
                                # Apply semantic corrections
                                corrected_pairs = await self._fix_common_semantic_mismatches(
                                    word_pairs, 
                                    is_german=is_german
                                )
                                
                                # Store the corrected pairs
                                style_entry['word_pairs'] = corrected_pairs
                                print(f"‚úÖ Added {len(corrected_pairs)} semantically-corrected general word pairs to {style_name}")

            print(f"‚úÖ Extracted {len(result['translations'])} translations")
            print(f"‚úÖ Extracted {len(result['style_data'])} style entries")
            
        except Exception as e:
            print(f"‚ùå Error in extraction: {str(e)}")
            import traceback
            traceback.print_exc()
            
            # Fallback: create minimal result
            if not result['translations']:
                result['translations'] = [generated_text[:500]]
                result['style_data'] = [{
                    'translation': generated_text[:500],
                    'word_pairs': [],
                    'is_german': False,
                    'is_spanish': False,
                    'style_name': 'fallback'
                }]
        
        # CRITICAL NEW ADDITION: Apply fallback word-by-word generation if needed
        result = self._ensure_word_by_word_data(result, style_preferences)
        
        # CRITICAL ADDITION: Ensure we have complete sentence translations for each enabled style
        result = self._ensure_complete_translations(result, style_preferences, generated_text)

        return result


    def _extract_translation_from_line(self, line: str, prefix: str) -> Optional[str]:
        """Extract translation text from a line with improved parsing"""
        try:
            # Remove the prefix and clean up
            translation = line.replace(prefix, '').strip()
            
            # Remove common brackets and quotes, but be more thorough
            translation = translation.strip('[]"\'').strip()
            
            # Remove patterns like "here]" or other common AI artifacts
            translation = re.sub(r'\[.*?\]$', '', translation).strip()
            translation = re.sub(r'^\[.*?\]\s*', '', translation).strip()
            
            # Remove "Provide xyz translation here" patterns
            if 'translation here' in translation.lower():
                return None
            
            # Must have substantial content and not just placeholder text
            if len(translation) > 3 and not translation.lower().startswith('provide'):
                return translation
            
        except Exception as e:
            print(f"‚ùå Error extracting from line '{line}': {str(e)}")
        
        return None


    def _parse_word_by_word_line(self, line: str) -> List[Tuple[str, str]]:
        """Parse a word-by-word line into pairs with context-aware semantic validation."""
        pairs = []
        
        try:
            # Find all [word] ([translation]) patterns
            pattern = r'\[([^\]]+)\]\s*\(\s*\[?([^)\]]+)\]?\s*\)'
            matches = re.findall(pattern, line)
            
            if not matches:
                # Try simpler pattern without inner brackets
                pattern = r'\[([^\]]+)\]\s*\(\s*([^)]+)\s*\)'
                matches = re.findall(pattern, line)
            
            for source, target in matches:
                source = source.strip()
                target = target.strip()
                if source and target:
                    # Remove any leftover brackets in the extracted values
                    source = source.strip('[]')
                    target = target.strip('[]')
                    
                    # Normalize the target - handle slash-separated alternatives
                    normalized_target = target
                    if '/' in target:
                        # Take first alternative by default, could be enhanced to take most appropriate
                        normalized_target = target.split('/')[0].strip()
                    
                    pairs.append((source, normalized_target))
                    print(f"   Pair: '{source}' -> '{normalized_target}'")
            
            # Contextual semantic validation that respects language variations
            if len(pairs) > 1:
                issues = self._validate_semantic_integrity(pairs)
                if issues:
                    for issue in issues:
                        print(f"‚ö†Ô∏è {issue}")
                    print("‚ö†Ô∏è Some semantic mismatches detected, but continuing with best effort")
                
        except Exception as e:
            print(f"Error parsing word-by-word line: {str(e)}")
        
        return pairs



    def _validate_semantic_integrity(self, pairs: List[Tuple[str, str]]) -> List[str]:
        """
        Validate semantic integrity of word pairs with context awareness.
        Returns a list of issues found.
        """
        issues = []
        
        # Get language context from the pairs
        # Is this German or English based on the source words?
        common_german_words = {"ich", "mein", "meine", "du", "ist", "sind", "habe", "haben", "der", "die", "das"}
        common_english_words = {"i", "my", "you", "is", "are", "have", "has", "the", "a", "an"}
        
        source_words = [pair[0].lower() for pair in pairs]
        german_matches = sum(1 for word in source_words if word in common_german_words)
        english_matches = sum(1 for word in source_words if word in common_english_words)
        
        is_likely_german = german_matches > english_matches
        language = "German" if is_likely_german else "English"
        
        # More flexible semantic validation with language-specific context
        semantic_context = {}
        
        # Extract verb forms to understand sentence structure
        for source, target in pairs:
            source_lower = source.lower()
            
            # Track verb forms to understand overall structure
            if is_likely_german:
                if source_lower in ["bin", "ist", "sind"]:
                    semantic_context["verb_form"] = "be"
                elif source_lower in ["habe", "hat", "haben"]:
                    semantic_context["verb_form"] = "have"
            else:  # English
                if source_lower in ["am", "is", "are"]:
                    semantic_context["verb_form"] = "be"
                elif source_lower in ["have", "has", "had"]:
                    semantic_context["verb_form"] = "have"
        
        # Specific validations based on detected context
        for source, target in pairs:
            source_lower = source.lower()
            target_lower = target.lower()
            
            # Verb form validation with context
            if semantic_context.get("verb_form") == "be":
                if is_likely_german and source_lower in ["bin", "ist", "sind"]:
                    # Allow both ser and estar forms in Spanish depending on context
                    if target_lower not in ["soy", "es", "son", "estoy", "est√°", "est√°n"]:
                        issues.append(f"Semantic mismatch in {language}: '{source}' should translate to a form of 'ser' or 'estar', got '{target}'")
                elif not is_likely_german and source_lower in ["am", "is", "are"]:
                    if target_lower not in ["soy", "eres", "es", "somos", "son", "estoy", "est√°s", "est√°", "estamos", "est√°n"]:
                        issues.append(f"Semantic mismatch in {language}: '{source}' should translate to a form of 'ser' or 'estar', got '{target}'")
            
            # Possessives validation
            if is_likely_german and source_lower in ["mein", "meine"]:
                if target_lower not in ["mi", "mis"]:
                    issues.append(f"Semantic mismatch in {language}: '{source}' should translate to 'mi/mis', got '{target}'")
            elif not is_likely_german and source_lower == "my":
                if target_lower not in ["mi", "mis"]:
                    issues.append(f"Semantic mismatch in {language}: '{source}' should translate to 'mi/mis', got '{target}'")
            
            # Pronoun validation
            if is_likely_german and source_lower == "ich":
                if target_lower != "yo":
                    issues.append(f"Semantic mismatch in {language}: '{source}' should translate to 'yo', got '{target}'")
            elif not is_likely_german and source_lower == "i":
                if target_lower != "yo":
                    issues.append(f"Semantic mismatch in {language}: '{source}' should translate to 'yo', got '{target}'")
        
        return issues
    
    def _apply_semantic_corrections(self, pairs: List[Tuple[str, str]], language: str = "auto") -> List[Tuple[str, str]]:
        """
        Apply semantic corrections to fix detected mismatches.
        Returns corrected word pairs that users will see and hear.
        """
        corrected_pairs = []
        corrections_made = 0
        
        # Determine if we're dealing with German or English source
        is_likely_german = self._is_likely_german_source(pairs)
        
        # Semantic correction mappings
        corrections = {
            # German corrections
            'german': {
                'ich': 'yo',  # I
                'bin': 'soy',  # am (permanent states)
                'ist': 'es',   # is
                'sind': 'son', # are
                'habe': 'tengo', # have
                'mein': 'mi',   # my
                'meine': 'mi',  # my (feminine)
                'und': 'y',     # and
                'der': 'el',    # the (masculine)
                'die': 'la',    # the (feminine)
                'das': 'la',    # the (neuter -> feminine in Spanish)
            },
            # English corrections
            'english': {
                'i': 'yo',      # I
                'am': 'soy',    # am
                'is': 'es',     # is
                'are': 'son',   # are
                'have': 'tengo', # have
                'my': 'mi',     # my
                'and': 'y',     # and
                'the': 'la',    # the (default to feminine)
            }
        }
        
        source_lang = 'german' if is_likely_german else 'english'
        correction_map = corrections.get(source_lang, {})
        
        for source, target in pairs:
            source_lower = source.lower()
            target_lower = target.lower()
            
            # Check if this word needs correction
            if source_lower in correction_map:
                correct_translation = correction_map[source_lower]
                
                # Only correct if the current translation is wrong
                if target_lower != correct_translation.lower():
                    corrected_pairs.append((source, correct_translation))
                    corrections_made += 1
                    print(f"SEMANTIC CORRECTION: {source} -> '{target}' corrected to '{correct_translation}'")
                else:
                    # Translation is already correct
                    corrected_pairs.append((source, target))
            else:
                # No correction needed for this word
                corrected_pairs.append((source, target))
        
        if corrections_made > 0:
            print(f"‚úÖ Applied {corrections_made} semantic corrections for better accuracy")
        
        return corrected_pairs


    def _detect_position_based_matches(self, pairs: List[Tuple[str, str]], source_language: str = "spanish") -> bool:
        """
        Detect if pairs are likely position-based matches rather than semantic matches.
        Returns True if position-based matching is suspected.
        Much more flexible and context-aware, especially for languages with multiple "to be" verbs.
        """
        # Only perform rudimentary checks for really obvious mismatches
        # Rather than trying to build a comprehensive dictionary, focus on basic patterns
        
        incorrect_pairings = []
        
        # Check for obviously incorrect matches like pronouns paired with nouns
        for source, target in pairs:
            source_lower = source.lower()
            target_lower = target.lower()
            
            # These are definitely wrong regardless of language
            if source_lower in ["ich", "i"] and target_lower not in ["yo", "i", "me"]:
                if not target_lower.startswith("m"): # Allow for "mi" "mis" etc.
                    incorrect_pairings.append((source, target))
                    
            # Check for really obvious noun/pronoun mismatches
            elif source_lower in ["h√§nde", "hands"] and target_lower not in ["manos", "hands", "hand"]:
                incorrect_pairings.append((source, target))
        
        # If more than one pairing is obviously wrong, flag it
        if len(incorrect_pairings) > 1:
            for source, target in incorrect_pairings:
                print(f"‚ö†Ô∏è Likely incorrect match: '{source}' ‚Üí '{target}'")
            return True
        
        return False
   
    # Add a method to check formatting consistency without validation dictionaries
    def _check_format_consistency(self, pairs: List[Tuple[str, str]]) -> None:
        """Check if the pairs have consistent formatting without using dictionaries."""
        if not pairs or len(pairs) < 2:
            return
        
        # Check format consistency only
        has_bracket_issues = False
        for source, target in pairs:
            if '[' in source or ']' in source or '(' in source or ')' in source:
                has_bracket_issues = True
                print(f"‚ö†Ô∏è Format issue: Source '{source}' contains brackets")
            
            if '[' in target or ']' in target or '(' in target or ')' in target:
                has_bracket_issues = True
                print(f"‚ö†Ô∏è Format issue: Target '{target}' contains brackets")
        
        if has_bracket_issues:
            print("‚ö†Ô∏è Format inconsistencies detected - this may affect audio playback")



    def _create_perfect_ui_sync_data(self, translations_data: Dict, style_preferences) -> Dict[str, Dict[str, str]]:
        """Create UI data that PERFECTLY matches what will be spoken in audio for ALL styles with minimal validation"""
        ui_data = {}
        
        print("üì± Creating PERFECT MULTI-STYLE UI-Audio synchronization data...")
        print("="*60)
        
        style_counter = {}  # Track counters per style
        
        for style_info in translations_data.get('style_data', []):
            style_name = style_info['style_name']
            word_pairs = style_info.get('word_pairs', [])
            is_german = style_info.get('is_german', False)
            is_spanish = style_info.get('is_spanish', False)
            
            # Initialize counter for this style
            if style_name not in style_counter:
                style_counter[style_name] = 0
            
            # Check if word-by-word is enabled for this language
            should_include = False
            if is_german and getattr(style_preferences, 'german_word_by_word', False):
                should_include = True
            elif not is_german and not is_spanish and getattr(style_preferences, 'english_word_by_word', False):
                should_include = True
            
            if should_include and word_pairs:
                print(f"üîÑ PERFECT SYNC: {style_name} with {len(word_pairs)} pairs")
                
                for i, (source_word, spanish_equiv) in enumerate(word_pairs):
                    # Clean for consistency
                    source_clean = source_word.strip().strip('"\'[]')
                    spanish_clean = spanish_equiv.strip().strip('"\'[]')
                    
                    # CRITICAL: Create EXACT same format for UI and audio
                    display_format = f"[{source_clean}] ([{spanish_clean}])"
                    
                    # Create unique key for each style's word pairs
                    key = f"{style_name}_{i}_{source_clean.replace(' ', '_')}"
                    
                    ui_data[key] = {
                        "source": source_clean,
                        "spanish": spanish_clean,
                        "language": "german" if is_german else "english",
                        "style": style_name,
                        "order": str(style_counter[style_name]),
                        "is_phrasal_verb": str(" " in source_clean),
                        "display_format": display_format  # EXACT audio format
                    }
                    
                    style_counter[style_name] += 1
                    
                    print(f"    {i+1}. {style_name} UI: {display_format}")
                    if " " in source_clean:
                        verb_type = "German Separable Verb" if is_german else "English Phrasal Verb"
                        print(f"       üîó {verb_type}: Single unit")
        
        print(f"‚úÖ Created PERFECT UI sync data for {len(ui_data)} word pairs across {len(style_counter)} styles")
        print("="*60)
        return ui_data


    def _create_formatted_translation_text(self, translations_data: Dict) -> str:
        """Create nicely formatted translation text for display with all styles"""
        formatted_parts = []
        
        formatted_parts.append("=" * 50)
        formatted_parts.append("MULTI-STYLE TRANSLATION RESULTS:")
        formatted_parts.append("=" * 50)
        
        # Group by language
        german_translations = []
        english_translations = []
        spanish_translations = []
        
        for style_info in translations_data.get('style_data', []):
            translation = style_info['translation']
            style_name = style_info['style_name']
            is_german = style_info.get('is_german', False)
            is_spanish = style_info.get('is_spanish', False)
            
            if is_german:
                german_translations.append(f"* {style_name.replace('_', ' ').title()}: {translation}")
            elif is_spanish:
                spanish_translations.append(f"* {style_name.replace('_', ' ').title()}: {translation}")
            else:
                english_translations.append(f"* {style_name.replace('_', ' ').title()}: {translation}")
        
        # Add German section
        if german_translations:
            formatted_parts.append("\nGERMAN TRANSLATIONS:")
            formatted_parts.append("-" * 25)
            formatted_parts.extend(german_translations)
        
        # Add English section
        if english_translations:
            formatted_parts.append("\nENGLISH TRANSLATIONS:")
            formatted_parts.append("-" * 25)
            formatted_parts.extend(english_translations)
        
        # Add Spanish section
        if spanish_translations:
            formatted_parts.append("\nSPANISH TRANSLATIONS:")
            formatted_parts.append("-" * 25)
            formatted_parts.extend(spanish_translations)
        
        formatted_parts.append("\n" + "=" * 50)
        
        return "\n".join(formatted_parts)

    async def _create_styles_data(self, translations_data: Dict) -> List[Dict]:
        """AI-POWERED styles data creation with neural optimizer integration
        
        CRITICAL: This creates the styles field with AI-generated word-by-word translations,
        confidence scores, and detailed explanations for language learning.
        """
        styles_data = []
        original_text = translations_data.get('original_text', '')
        
        # Import the high-speed neural optimizer
        try:
            from .high_speed_neural_optimizer import high_speed_neural_optimizer
            print(f"ü§ñ Using AI Neural Optimizer for word-by-word translations")
        except ImportError:
            print(f"‚ö†Ô∏è Neural optimizer not available, using fallback")
            return await self._create_styles_data_fallback(translations_data)
        
        for style_info in translations_data.get('style_data', []):
            style_name = style_info.get('style_name', '')
            translation_text = style_info.get('translation', '')
            
            # IMPORTANT: Ensure we have a complete translation, never empty
            if not translation_text or translation_text.strip() == '':
                # Generate fallback translation
                language = 'German' if 'german' in style_name else 'English'
                style_type = style_name.split('_')[-1].title() if '_' in style_name else 'Standard'
                
                if original_text:
                    translation_text = f"Complete {language} {style_type} translation of: '{original_text}'"
                else:
                    translation_text = f"{language} {style_type} translation available"
                
                print(f"‚ö†Ô∏è Generated fallback translation for {style_name}: {translation_text}")
            
            # ü§ñ AI-POWERED WORD-BY-WORD TRANSLATION
            word_pairs_formatted = []
            
            if translation_text and original_text:
                try:
                    # Determine source and target languages
                    source_language = 'german' if 'german' in style_name.lower() else 'english'
                    target_language = 'spanish'  # User's mother tongue
                    style_type = style_name.split('_')[-1] if '_' in style_name else 'native'
                    
                    print(f"üß† Calling Neural Optimizer for {style_name}: {translation_text[:50]}...")
                    
                    # Call the high-speed neural optimizer for AI translations
                    neural_result = await high_speed_neural_optimizer.optimize_word_by_word_translation(
                        source_text=translation_text,
                        source_language=source_language,
                        target_language=target_language,
                        style=style_type
                    )
                    
                    print(f"‚úÖ Neural Optimizer returned {len(neural_result.word_mappings)} AI mappings")
                    print(f"üìä Average confidence: {neural_result.average_confidence:.2f}")
                    
                    # Convert neural optimizer results to frontend format
                    for i, mapping in enumerate(neural_result.word_mappings):
                        word_pairs_formatted.append({
                            'source': mapping.source_phrase,
                            'spanish': mapping.target_phrase,
                            'order': i,
                            'confidence': mapping.confidence,
                            'explanation': getattr(mapping, 'explanation', ''),
                            'type': getattr(mapping, 'word_type', 'word'),
                            'is_phrasal_verb': ' ' in mapping.source_phrase
                        })
                        
                        print(f"üéØ AI Translation: {mapping.source_phrase} ‚Üí {mapping.target_phrase} ({mapping.confidence:.2f})")
                        if hasattr(mapping, 'explanation') and mapping.explanation:
                            print(f"üìù Explanation: {mapping.explanation}")
                    
                except Exception as neural_error:
                    print(f"‚ùå Neural optimizer failed for {style_name}: {neural_error}")
                    # Fallback to basic word pairs if AI fails
                    word_pairs_raw = style_info.get('word_pairs', [])
                    for i, pair in enumerate(word_pairs_raw):
                        if isinstance(pair, (list, tuple)) and len(pair) >= 2:
                            source_word = str(pair[0]).strip().strip('"\'[]')
                            spanish_word = str(pair[1]).strip().strip('"\'[]')
                            
                            word_pairs_formatted.append({
                                'source': source_word,
                                'spanish': spanish_word,
                                'order': i,
                                'confidence': 0.85,  # Default confidence
                                'explanation': '',
                                'type': 'word',
                                'is_phrasal_verb': ' ' in source_word
                            })
            
            # Create the style data structure with AI enhancements
            style_data = {
                'name': style_name,
                'translation': translation_text,
                'word_pairs': word_pairs_formatted,
                'has_word_by_word': len(word_pairs_formatted) > 0,
                'ai_powered': True,
                'confidence_average': sum(wp.get('confidence', 0.85) for wp in word_pairs_formatted) / len(word_pairs_formatted) if word_pairs_formatted else 0.85
            }
            
            styles_data.append(style_data)
            print(f"ü§ñ AI Style created: {style_name} - {len(word_pairs_formatted)} AI word pairs")
        
        print(f"üöÄ Created {len(styles_data)} AI-powered styles with neural translations")
        return styles_data
    
    async def _create_styles_data_fallback(self, translations_data: Dict) -> List[Dict]:
        """Fallback method when neural optimizer is not available"""
        print("‚ö†Ô∏è Using fallback styles creation (no AI)")
        # Keep the original logic as fallback
        styles_data = []
        
        for style_info in translations_data.get('style_data', []):
            style_name = style_info.get('style_name', '')
            translation_text = style_info.get('translation', '')
            word_pairs_raw = style_info.get('word_pairs', [])
            
            word_pairs_formatted = []
            for i, pair in enumerate(word_pairs_raw):
                if isinstance(pair, (list, tuple)) and len(pair) >= 2:
                    word_pairs_formatted.append({
                        'source': str(pair[0]).strip(),
                        'spanish': str(pair[1]).strip(),
                        'order': i,
                        'confidence': 0.85,
                        'explanation': '',
                        'type': 'word'
                    })
            
            styles_data.append({
                'name': style_name,
                'translation': translation_text,
                'word_pairs': word_pairs_formatted,
                'has_word_by_word': len(word_pairs_formatted) > 0
            })
        
        return styles_data

    # Keep all other existing methods unchanged...
    def _generate_word_by_word(self, original: str, translated: str) -> dict[str, dict[str, str]]:
        """Generate word-by-word translation mapping."""
        result = {}
        original_words = original.split()
        translated_words = translated.split()

        for i, word in enumerate(original_words):
            if i < len(translated_words):
                result[word] = {
                    "translation": translated_words[i],
                    "pos": "unknown",
                }
        return result

    def _generate_grammar_explanations(self, text: str) -> dict[str, str]:
        """Generate grammar explanations for the translation."""
        return {
            "structure": "Enhanced multi-style grammar explanation with context",
            "tense": "Tense usage adapted to multiple styles and mother tongue",
        }

    def _normalize_text(self, text: str) -> str:
        normalized = unicodedata.normalize("NFKD", text)
        ascii_text = normalized.encode("ascii", "ignore").decode("ascii")
        return ascii_text

    def _restore_accents(self, text: str) -> str:
        accent_map = {
            "a": "√°", "e": "√©", "i": "√≠", "o": "√≥", "u": "√∫", "n": "√±",
            "A": "√Å", "E": "√â", "I": "√ç", "O": "√ì", "U": "√ö", "N": "√ë",
        }

        patterns = {
            r"([aeiou])¬¥": lambda m: accent_map[m.group(1)],
            r"([AEIOU])¬¥": lambda m: accent_map[m.group(1)],
            r"n~": "√±", r"N~": "√ë",
        }

        for pattern, replacement in patterns.items():
            if callable(replacement):
                text = re.sub(pattern, replacement, text)
            else:
                text = re.sub(pattern, replacement, text)

        return text

    def _ensure_unicode(self, text: str) -> str:
        if isinstance(text, bytes):
            text = text.decode("utf-8")
        return unicodedata.normalize("NFKC", text)

    def _get_temp_directory(self) -> str:
        """Get the appropriate temporary directory based on the operating system."""
        if os.name == "nt":
            temp_dir = os.environ.get("TEMP") or os.environ.get("TMP")
        else:
            temp_dir = "/tmp"

        os.makedirs(temp_dir, exist_ok=True)
        return temp_dir

    def _auto_fix_spelling(self, text: str) -> str:
        """Fix spelling in the given text."""
        words = re.findall(r"\b\w+\b|[^\w\s]", text)
        corrected_words = []

        for word in words:
            if not re.match(r"\w+", word):
                corrected_words.append(word)
                continue

            if self.spell.unknown([word]):
                correction = self.spell.correction(word)
                if correction:
                    if word.isupper():
                        correction = correction.upper()
                    elif word[0].isupper():
                        correction = correction.capitalize()
                    word = correction

            corrected_words.append(word)

        return " ".join(corrected_words)
    
    async def translate_with_universal_ai(
        self,
        text: str,
        target_language: str,
        mother_tongue: str = None,
        style: str = 'native'
    ) -> Translation:
        """
        Universal AI-powered translation with dynamic word-by-word alignment
        Uses Gemini AI for intelligent, context-aware translations
        """
        try:
            # Auto-detect source language if not provided
            detected_source = await universal_ai_translator.detect_language(text)
            source_language = detected_source if detected_source != "unknown" else "auto"
            
            print(f"üåç Universal AI Translation: {source_language} ‚Üí {target_language}")
            print(f"üìù Input: {text}")
            
            # Get universal AI translation with word alignment
            ai_result = await universal_ai_translator.translate_with_word_alignment(
                text=text,
                source_language=source_language,
                target_language=target_language,
                style=style
            )
            
            # Convert word mappings to the format expected by the Translation entity
            word_by_word = {}
            for mapping in ai_result.word_mappings:
                word_by_word[mapping.source_phrase] = {
                    'translation': mapping.target_phrase,
                    'confidence': mapping.confidence,
                    'type': mapping.phrase_type,
                    'word_count': mapping.word_count
                }
            
            # Generate audio for main translation
            audio_path = None
            try:
                audio_path = await self._generate_audio_for_translation(
                    ai_result.translated_text, 
                    target_language, 
                    style
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Audio generation failed: {e}")
            
            # Create Translation object
            translation = Translation(
                original_text=text,
                translated_text=ai_result.translated_text,
                source_language=source_language,
                target_language=target_language,
                audio_path=audio_path,
                word_by_word=word_by_word
            )
            
            # Log AI confidence ratings (internal only)
            print(f"‚úÖ Universal AI Translation completed with {ai_result.overall_confidence:.2f} overall confidence")
            print(f"‚ö° Processing time: {ai_result.processing_time:.2f}s")
            print(f"üéµ Word mappings generated: {len(ai_result.word_mappings)}")
            
            return translation
            
        except Exception as e:
            print(f"‚ùå Universal AI translation failed: {e}")
            # Fallback to regular translation
            return await self._fallback_translation(text, target_language, mother_tongue, style)
    
    async def _fallback_translation(
        self,
        text: str,
        target_language: str,
        mother_tongue: str = None,
        style: str = 'native'
    ) -> Translation:
        """Fallback translation when universal AI fails"""
        
        try:
            # Use the existing process_prompt method as fallback
            base_service = TranslationService()
            return await base_service.process_prompt(
                text=text,
                source_lang="auto",
                target_lang=target_language,
                mother_tongue=mother_tongue
            )
        except Exception as e:
            print(f"‚ùå Fallback translation also failed: {e}")
            # Last resort: simple translation
            return Translation(
                original_text=text,
                translated_text=f"[Translation of: {text}]",
                source_language="auto",
                target_language=target_language,
                audio_path=None,
                word_by_word={}
            )
    
    async def _generate_audio_for_translation(
        self,
        text: str,
        language: str,
        style: str = 'native'
    ) -> Optional[str]:
        """Generate audio for the translated text"""
        
        try:
            # Map language codes
            language_mapping = {
                'spanish': 'es',
                'english': 'en',
                'german': 'de',
                'french': 'fr',
                'italian': 'it',
                'portuguese': 'pt',
                'russian': 'ru',
                'chinese': 'zh',
                'japanese': 'ja',
                'korean': 'ko',
                'arabic': 'ar',
                'hindi': 'hi'
            }
            
            lang_code = language_mapping.get(language.lower(), language)
            
            # Generate audio using TTS service
            audio_path = await self.tts_service.generate_speech(
                text=text,
                language=lang_code,
                voice_style=style
            )
            
            return audio_path
            
        except Exception as e:
            print(f"‚ö†Ô∏è Audio generation failed: {e}")
            return None
    
